import os
from langchain.agents import initialize_agent
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.agents import AgentType
from openai import NotFoundError
from src.langchain_tools import broad_insights_tools, deep_dive_tools

llm = None
agent_executor = None
current_model = None


def init_llm_and_executor(model_name: str = "openai/gpt-4o") -> None:
    """
    Initializes the LangChain agent with OpenRouter.ai API and runs an initial test query.

    Args:
        model_name: The model name to use (default: openai/gpt-4o)

    Raises:
        AuthenticationError: If the API key is invalid.
        RateLimitError: If OpenRouter usage limits are exceeded.
        Exception: For any other unexpected errors.
    """
    global llm, agent_executor, current_model

    # Decide provider: OpenRouter (default) or LM Studio local server
    provider = os.getenv("LLM_PROVIDER", "openrouter").strip().lower()

    if provider == "lmstudio":
        # LM Studio runs an OpenAI-compatible server locally, typically at http://localhost:1234/v1
        base_url = os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
        # LM Studio usually accepts any API key string; provide a default placeholder
        api_key = ""

        llm = ChatOpenAI(
            model_name=model_name,
            openai_api_key=api_key,
            openai_api_base=base_url,
        )
    else:
        # Default to OpenRouter
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        llm = ChatOpenAI(
            model_name=model_name,
            openai_api_key=api_key,
            openai_api_base="https://openrouter.ai/api/v1",
        )

    try:
        llm.invoke("Hi, this is a test query")
    except NotFoundError as e:
        # Only attempt OpenRouter fallback if using OpenRouter
        if os.getenv("LLM_PROVIDER", "openrouter").strip().lower() != "lmstudio":
            print(f"{model_name} doesn't exist, using openai/gpt-4o-mini instead")
            llm = ChatOpenAI(
                model_name="openai/gpt-4o-mini",
                openai_api_key=os.getenv("OPENAI_API_KEY"),
                openai_api_base="https://openrouter.ai/api/v1",
            )
            llm.invoke("Hi, this is a test query")
        else:
            # For LM Studio, surface the error (model likely not loaded or name mismatch)
            raise

    # Build prompt text dynamically to optionally include formatting rules for LM Studio
    rules_block = (
        """
            
            IMPORTANT FORMATTING RULES:
            - Always include "Thought:" before reasoning
            - Always include "Action:" before tool name  
            - Always include "Action Input:" before parameters
            - When you have a final answer, use: Final Answer: [your answer]
            
            ---
            
        """
        if provider == "lmstudio" else ""
    )

    prompt_text = (
        """
            You are an **AI Kubernetes Troubleshooting Assistant**.

            üîπ **You have access to two categories of tools**:
            1Ô∏è‚É£ **Broad Insights Tools** ‚Üí Surface-level data for multiple resources.
            2Ô∏è‚É£ **Deep Dive Tools** ‚Üí Detailed analysis of specific resources.

            ---

            **üìå How to use these tools:**
            - **Start with Broad Insights** ‚Üí Use **broad tools** if the user asks about overall cluster health.
            - **Use Deep Dive Tools** ‚Üí If an issue is suspected, analyze a specific resource in-depth.
            - **Correlate multiple tool outputs** to provide insightful recommendations.

            ---

            **üîπ Available Tools:**

            üü¢ **Broad Insights Tools**
            - "Get All Pods with Resource Usage" ‚Üí Lists all pods with CPU & Memory usage.
            - "Get All Services" ‚Üí Lists all services with their types and ports.
            - "Get All Deployments" ‚Üí Lists deployments with replica status.
            - "Get All Nodes" ‚Üí Lists nodes with their health & capacity.
            - "Get All Endpoints" ‚Üí Fetches endpoints and associated services.
            - "Get Cluster Events" ‚Üí Lists recent cluster-wide warnings & failures.
            - "Get Namespace List" ‚Üí Lists all namespaces and their statuses.

            üîµ **Deep Dive Tools**
            - "Describe Pod with Restart Count" ‚Üí Fetches detailed pod info + restart count.
            - "Get Pod Logs" ‚Üí Fetches the last 10 log lines for a specific pod.
            - "Describe Service" ‚Üí Fetches service details (ClusterIP, NodePort, ExternalName).
            - "Describe Deployment" ‚Üí Fetches replica counts & container images.
            - "Get Node Status & Capacity" ‚Üí Fetches node health conditions & resource usage.
            - "Check RBAC Events & Role Bindings" ‚Üí Fetches RBAC security events & role bindings.
            - "Get Persistent Volumes & Claims" ‚Üí Lists PVs, PVCs, and their bound claims.
            - "Get Running Jobs & CronJobs" ‚Üí Lists active Jobs & CronJobs.
            - "Get Ingress Resources & Annotations" ‚Üí Lists Ingress rules, hosts, and annotations.
            - "Check Pod Affinity & Anti-Affinity" ‚Üí Analyzes pod scheduling constraints.
            - "Get Kubernetes Object YAML" ‚Üí Fetches the complete YAML representation of any Kubernetes object (pod, deployment, service, configmap, secret, etc.)

            ---

            **üöÄ General Workflow for Debugging:
            1Ô∏è‚É£ Identify affected resources.
            2Ô∏è‚É£ Use **Broad Insights Tools** for an overview.
            3Ô∏è‚É£ Use **Deep Dive Tools** for root cause analysis.
            4Ô∏è‚É£ Provide actionable recommendations.

            ---

            **üöÄ Example Workflow for a Pod Issue:**
            1Ô∏è‚É£ **User Query:** "Why is my app crashing?"
            2Ô∏è‚É£ **Step 1:** Use "Get All Pods with Resource Usage" to identify failing pods.
            3Ô∏è‚É£ **Step 2:** Use "Describe Pod with Restart Count" for specific failure reasons.
            4Ô∏è‚É£ **Step 3:** Use "Get Pod Logs" to analyze errors.
            5Ô∏è‚É£ **Step 4:** If node issues are suspected, use "Get Node Status & Capacity".
            6Ô∏è‚É£ **Step 5:** Provide an **actionable recommendation** based on tool outputs.

            **üöÄ Example Workflow for a Advenced Pod Issue:**
            1Ô∏è‚É£ **User Query:** "Why is my app crashing?"
            2Ô∏è‚É£ **Step 1:** Use "Get All Pods with Resource Usage" to identify failing pods.
            3Ô∏è‚É£ **Step 2:** Use "Describe Pod with Restart Count" for specific failure reasons.
            4Ô∏è‚É£ **Step 3:** Use "Get Pod Logs" to analyze errors.
            5Ô∏è‚É£ **Step 4:** Use "Get Kubernetes Object YAML" to check if YAML is valid and no parameters are missed according to previous fetched logs.
            6Ô∏è‚É£ **Step 5:** Provide an **actionable recommendation** based on tool outputs.
        """
        + rules_block
        + """
            **üîπ User Query:** {input}
            """
    )

    # Define a Prompt Template for guiding the agent
    prompt_template = PromptTemplate(
        input_variables=["input"],
        template=prompt_text,
    )

    # Conversation memory to maintain context
    memory = ConversationBufferMemory(memory_key="chat_history")

    # Initialize the LangChain Agent with Kubernetes tools
    agent_executor = initialize_agent(
        tools=broad_insights_tools + deep_dive_tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        memory=memory,
        prompt=prompt_template,
        handle_parsing_errors=True
    )
    
    # Store the current model name
    current_model = model_name

def process_query(user_query: str, model_name: str = "openai/gpt-4o"):
    """Process natural language queries and fetch Kubernetes data via LangChain."""
    global current_model
    
    # Initialize if not done yet, or reinitialize if model changed
    if agent_executor is None or current_model != model_name:
        init_llm_and_executor(model_name)
    
    return agent_executor.invoke(user_query)
